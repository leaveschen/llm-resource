## Scaling law

### Original paper

https://arxiv.org/pdf/2001.08361.pdf

摘要：本文讨论了关于语言模型性能在交叉熵损失上的经验性 scaling-law。它指出损失与模型大小、数据集大小以及用于训练的计算量之间存在幂律关系，这些趋势跨越了七个数量级以上。网络宽度或深度等其他架构细节在广泛范围内对性能影响较小。简单的方程式描述了过拟合与模型/数据集大小的关系以及训练速度与模型大小的依赖关系。这些关系可以帮助确定固定计算预算的最佳分配。较大的模型在样本效率上更高，因此，最佳的计算效率训练涉及在相对较少的数据上训练非常大的模型，并在收敛之前明显停止训练。

核心结论如下：

**性能强依赖于规模，模型 shape 影响较小**：规模包括三个因素：模型参数数量N（不包括嵌入层），数据集大小D，以及用于训练的计算量C

**平滑的幂律关系**：当没有受到其他两个因素的限制时，性能与每个规模因素N、D、C都呈现出幂律关系

**过拟合的普适性**：只要我们同时增加N和D，性能就会可预测地提高，但如果其中一个保持不变而另一个增加，就会进入递减回报的阶段。性能损失可预测地取决于比率 $N^{0.74}/D$，这意味着每当我们将模型大小增加8倍时，我们只需要将数据大约增加5倍，就可以避免损失。

**训练的普适性**：训练曲线遵循可预测的幂律规律，其参数与模型大小大致独立。通过外推训练曲线的前期部分，我们可以粗略地预测如果我们进行更长时间的训练，可能达到的损失值。

**迁移学习与测试性能提高相关**：当我们在与训练分布不同的文本上评估模型时，结果与在训练验证集上的结果强相关，损失值之间存在一个大致恒定的偏差。换句话说，迁移到不同分布会带来一个恒定的惩罚，但在其他方面，迁移学习的表现大致与在训练集上的性能相似。

**样本效率**：较大的模型比小模型更具样本效率，在较少的优化步骤和使用较少的数据点的情况下就能达到相同的性能水平。

**收敛效率低下**：在一个固定的计算预算C的限制下，但没有对模型大小N或可用数据D的其他限制，通过训练非常大的模型并明显在收敛之前停止，我们可以达到最佳性能（见图3）。因此，最大化计算效率的训练要比基于训练小模型收敛的预期要更加样本效率，数据需求随着训练计算量的增加呈非常缓慢的增长，即 $D ∼ C^{0.27}$。

**最优 batch size**：训练这些模型的理想 batch size 大致是 power of the loss，

训练过程，使用 adam 优化器，这篇文章里最大模型只说是超过 1B（具体大小不明）使用 adafactor；学习率使用 3000 step 的 linear warmup，然后是 consine decay 到0

数据集主要是 WebText，总训练 token 有 $2.29\times10^{10}$ tokens
